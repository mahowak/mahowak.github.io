<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Kyle Mahowald</title>
  <meta name="description" content="Kyle Mahowald's Page">


  <link rel="stylesheet" href="css/tufte.css">	
  

  <!-- Google Fonts loaded here -->
  <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>

  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  

  <link rel="canonical" href="/">
  <link rel="alternate" type="application/rss+xml" title="Kyle Mahowald" href="/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
		<a href=""><img class="badge" src="assets/img/logo.png"></a>
		<a href="https://scholar.google.com/citations?user=XUmFLVUAAAAJ&hl=en">Google Scholar</a>
		<a href="https://docs.google.com/document/d/e/2PACX-1vSoRmuhEwwcePA795R4XhmIKjKlkSWV5KY6uRFRU7IudsFQVtinAj2b8_yisOtGdz9Qd4cKyz6s1JSS/pub">CV</a>
                <a href="https://osf.io/dvyq6">OSF</a>
                		<a href="http://www.xwordinfo.com/Thumbs?author=Kyle+Mahowald">Xword</a>
                                <a href="mailto:kyle@utexas.edu"><span class="icon-mail"></span></a>  <a href="http://www.twitter.com/kmahowald"><span class="icon-twitter"></span></a>
	</nav>
</header>

    <article>
      <h1></h1>
<p></p>


<p><span class="marginnote"></span><img class="fullwidth" src="assets/img/kylefogo2.png" /></p>

<h1 class="content-listing-header sans">Kyle Mahowald</h1>
<p><a href="mailto:mahowald@utexas.edu"> mahowald@utexas.edu</a>
<!--
 <a href="http://www.twitter.edu/kmahowald"><span class="icon-twitter"></span></a> [@kmahowald]("http://www.twitter.edu/kmahowald")
 --></p>

<h3 id="about-me">About me</h3>

<p><span class="newthought">I am an assistant professor</span>  in <a href="https://liberalarts.utexas.edu/linguistics/">linguistics</a> at University of Texas at Austin (formerly at UCSB). I’m accepting students, am a member of the <a href="https://sites.utexas.edu/compling/">Computational Linguistics Research Group</a> here at UT, and am part of the <a href="https://www.nlp.utexas.edu">wider UT Austin NLP community</a>.I am interested in what AI and computational language models can tell us about human language and the human mind, how linguistic efficiency can explain aspects of linguistic behavior and typology, and a variety of other topcis in the cognitive science of language. A major part of this work involves developing and using techniques for interpreting AI models. I also have interests in quantitative methods in the sciences. I am a winner of an NSF CAREER Award (2024-2029).</p>

<p>I graduated with my Ph.D. from <a href="http://tedlab.mit.edu/">Ted Gibson’s psycholinguistics lab</a> in MIT’s <a href="http://bcs.mit.edu/">Brain and Cognitive Sciences</a> department and did my postdoc with <a href="http://web.stanford.edu/~jurafsky/">Dan Jurafsky</a> (CS/Linguistics) and <a href="https://profiles.stanford.edu/daniel-mcfarland">Dan McFarland</a> (Graduate School of Education) at Stanford. Before that, I was a Marshall Scholar at Oxford University studying linguistics with <a href="http://users.ox.ac.uk/~cpgl0015/">Mary Dalrymple</a> and <a href="http://www.ling-phil.ox.ac.uk/lahiri">Aditi Lahiri</a>. And before that I was an undergrad in English at Harvard.<sup class="sidenote-number">1</sup><span class="sidenote"><sup class="sidenote-number">1</sup> About <em>me</em>: <em>Me</em> is the first person object pronoun in English. It goes back to the accusative and dative in Old English and before that to an Indo-European pronoun that would probably remind you of me.</span></p>

<p>I also make crosswords and wrote about what they have to do with linguistics <a href="https://www.theatlantic.com/science/archive/2023/08/writing-crossword-puzzle-clues-rules-grammar-compositionality/674938/">here</a> in The Atlantic, with Scott AnderBois and Nick Tomlin; and did a <a href="https://schedule.sxsw.com/2024/events/PP134394">SXSW panel on crosswords</a>. See also a <a href="https://theconversation.com/googles-powerful-ai-spotlights-a-human-cognitive-glitch-mistaking-fluent-speech-for-fluent-thought-185099">pop piece with Anna Ivanova on LLMs and thinking</a>.</p>

<h3 id="group-members-and-alumni">Group members and alumni</h3>

<p><a href="https://kanishka.website">Kanishka Misra</a>
<a href="https://leonieweissweiler.github.io">Leonie Weissweiler</a>
<a href="https://oden.utexas.edu/people/directory/William-Rudman/">William Rudman</a>
<a href="https://sashaboguraev.github.io">Sasha Boguraev</a>
<a href="https://dounick.github.io">Qing Yao</a>
<a href="https://joshbarua.github.io">Josh Barua</a>
<a href="https://siyuansong.site">Siyuan Song</a></p>

<h3 id="articles">Articles</h3>

<p>R. Futrell, K. Mahowald. 2025 (to appear). <a href="https://arxiv.org/abs/2501.17047">How Linguistics Learned to Stop Worrying and Love the Language Models</a>. <em>Brain and Behavioral Sciences</em> target article.</p>

<p>S. Boguraev, C. Potts, K. Mahowald. 2025. <a href="https://arxiv.org/abs/2505.16002">Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions</a>. EMNLP 2025.
<span class="marginnote"><img class="fullwidth" src="assets/img/causal_transfer_picture.png" />Boguraev et al. (2025) applies causal intervention techniques to LLMs and shows that diverse filler-gap constructions (e.g., <em>wh</em>-questions, relative clauses) share common internal structure representations, suggesting abstract grammatical convergence.</span></p>

<p>Q. Yao, K. Misra, L. Weissweiler, K. Mahowald. 2025. <a href="https://arxiv.org/abs/2503.20850">Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models</a>. COLM 2025.</p>

<p>S. Song, J. Hu, K. Mahowald. 2025. <a href="https://arxiv.org/abs/2503.07513">Language Models Fail to Introspect About Their Knowledge of Language</a>. COLM 2025.</p>

<p>L. Weissweiler, K. Mahowald, A. Goldberg. 2025. <a href="https://arxiv.org/abs/2502.13195">Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs</a>. arXiv preprint arXiv:2502.13195.</p>

<p>S. Padmanabhan, K. Misra, K. Mahowald, E. Choi. 2025. <a href="https://arxiv.org/abs/2504.09387">On Language Models’ Sensitivity to Suspicious Coincidences</a>. arXiv preprint arXiv:2504.09387.</p>

<p>J. Rozner, L. Weissweiler, K. Mahowald, C. Shain. 2025. <a href="https://arxiv.org/abs/2503.06048">Constructions are Revealed in Word Distributions</a>. arXiv preprint arXiv:2503.06048.</p>

<p>W. B. Sheffield, K. Misra, V. Pyatkin, A. Deo, K. Mahowald, J.J. Li. 2025. <a href="https://arxiv.org/pdf/2506.04534">Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs.</a> Findings of Association for Computational Linguistics (ACL) 2025.</p>

<p>J. Ranganathan, R. Jha, K. Misra, K. Mahowald. 2025. <a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=ranganathan+misra+mahowald&amp;ie=UTF-8&amp;oe=UTF-8&amp;safe=active">semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces</a>. SCiL 2025.</p>

<p>K Misra, K Mahowald. 2024. <a href="https://arxiv.org/abs/2403.19827">Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs</a>. EMNLP 2024. Outstanding Paper Award.<span class="marginnote"><img class="fullwidth" src="assets/img/aann.png" />Misra and Mahowald (2024) systematically trains small language models on controlled input corpora and studies the learning of the AANN “a beautiful five days” construction. Models can learn it even when the construction is absent, suggesting transfer from related constructions.</span></p>

<p>J. Kallini, I. Papadimitriou, R. Futrell, K. Mahowald, C. Potts. 2024. <a href="https://arxiv.org/abs/2401.06416">Mission: Impossible Language Models</a>. ACL 2024 (Best Paper Award).</p>

<p>H. Lederman, K. Mahowald. 2024. <a href="https://arxiv.org/abs/2401.04854">Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs</a>. TACL 2024.</p>

<p>K. Misra, A. Ettinger, K. Mahowald. 2024. <a href="https://arxiv.org/abs/2401.06640">Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently</a>. EMNLP 2024.</p>

<p>Z. Sprague, F. Yin, J. D. Rodriguez, D. Jiang, M. Wadhwa, P. Singhal, X. Zhao, X. Ye, K. Mahowald, G. Durrett. 2025. <a href="https://arxiv.org/abs/2409.12183">To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</a>. ICLR 2025.</p>

<p>Tuckute, G., Mahowald, K., Isola, P., Fedorenko, E., Gibson, E., and Oliva, A. 2025. <a href="http://tedlab.mit.edu/tedlab_website/researchpapers/tuckute_mahowald_et_al_2025.pdf">Intrinsically memorable words have unique associations with their meanings</a>.  Journal of Experimental Psychology General.</p>

<p>S. Boguraev, B. Lipkin, L. Weissweiler, K. Mahowald. 2024. <a href="https://arxiv.org/abs/2409.17005">Models Can and Should Embrace the Communicative Nature of Human-Generated Math</a>. The 4th Workshop on Mathematical Reasoning and AI (NeurIPS Workshop).</p>

<p>V. S. Govindarajan, M. Zang, K. Mahowald, D. Beaver, J. J. Li. 2024. <a href="https://arxiv.org/abs/2406.17947">Do they mean ‘us’? Interpreting Referring Expressions in Intergroup Bias</a>. Findings of EMNLP 2024.</p>

<p>J. Hu, K. Mahowald, G. Lupyan, A. Ivanova, R. Levy. 2024. <a href="https://arxiv.org/abs/2402.01676">Language models align with human judgments on key grammatical constructions</a>. <em>PNAS</em> letter.</p>

<p>K. Denlinger, S. Wechsler, K. Mahowald. 2024. <a href="https://arxiv.org/abs/2405.10457">Participle-Prepended Nominals Have Lower Entropy Than Nominals Appended After the Participle</a>. CogSci 2024.</p>

<p>T. Pimentel, C. Meister, E. G. Wilcox, K. Mahowald, R. Cotterell. 2023. <a href="https://aclanthology.org/2023.emnlp-main.137/">Revisiting the optimality of word lengths. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</a>, pages 2240–2255, Singapore, December 2023. Outstanding Paper Award.</p>

<p>A Srinivasan, VS Govindarajan, K. Mahowald. 2023. <a href="https://arxiv.org/pdf/2310.18862.pdf">Counterfactually probing language identity in multilingual models</a>. Proceedings of the 3rd Workshop on Multi-lingual Representation Learning (MRL), pages 24–36, Singapore, December 2023.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/langposition.png" />Mahowald, Ivanova et al. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective” gives a cognitive science perspective on Large Language Models, arguing for a distinction between formal competence and functional competence.</span>K. Mahowald, A. Ivanova, I. Blank, N. Kanwisher, J. Tenenbaum, E. Fedorenko. 2024. <a href="assets/pdf/mahowald2024dissociating.pdf">Dissociating Language and Thought in Large Language Models: A Cognitive Perspective</a>.<em>Trends in Cognitive Sciences</em>.</p>

<p>V. Govindarajan, J.D. Rodriguez, K. Bostrom, K. Mahowald. 2023. <a href="https://arxiv.org/pdf/2310.17591">Lil-bevo: Explorations of strategies for training language models in more humanlike ways</a>. Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 280–288, Singapore, December 2023.</p>

<p>M. Mersinias, K. Mahowald. 2023. <a href="https://arxiv.org/abs/2302.08577">For generated text, is NLI-neutral text the best text?</a> Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2596–2602, Singapore, December 2023.</p>

<p>T. Regev, J. Affourtit, X. Chen, A. Schipper, L. Bergen, L., K. Mahowald, E. Fedorenko. 2024. <a href="https://t.co/b54FKfZFWq">High-level language brain regions are sensitive to sub-lexical regularities</a>. <em>Cerebral Cortex</em>.</p>

<p>Mahowald, K., Diachek, E., Gibson, E., Fedorenko, E., &amp; Futrell, R. (2023). <a href="assets/pdf/mahowald_gramm_cues.pdf">Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages</a>. <em>Cognition</em>, 241, 105543.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/chen_fig.jpg" />Chen et al. (2023) looks at informativity and complexity tradeoffs in spatial adverbs like <em>here</em> and <em>there</em> across world languages and finds that languages tend to fall on an efficient frontier.</span>Chen, S., Futrell, R., &amp; Mahowald, K. (2023). <a href="https://sites.socsci.uci.edu/~rfutrell/papers/chen2023information.pdf">An information-theoretic approach to the typology of spatial demonstratives</a>. <em>Cognition</em>, 240, 105505.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/mahowald_aann.jpg" />This EACL paper explores the AANN “a beautiful five days” construction and shows a high degree of similarity between GPT-3 ratings and human ratings.</span>K. Mahowald. 2023. <a href="https://arxiv.org/abs/2301.12564">A Discerning Several Thousand Judgments: GPT-3 Rates the Article  Adjective + Numeral + Noun Construction</a>. Proceedings of EACL 2023.</p>

<p>G. Chronis, K. Mahowald, K. Erk. 2023. <a href="https://arxiv.org/abs/2305.18598">A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces</a>. Proceedings of ACL 2023.</p>

<p>J. Huang, Z. Wu, K. Mahowald, C. Potts. 2023. <a href="https://arxiv.org/abs/2212.09897">Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training</a>. Findings of ACL 2023.</p>

<p>V. Govindarajan, K. Mahowald, D. Beaver, J. Li. 2023. <a href="https://arxiv.org/abs/2305.16409">Counterfactual Probing for the influence of affect and specificity on Intergroup Bias</a>. Findings of ACL 2023.</p>

<p>Y. Wu, W. Sheffield, K. Mahowald, J. Li. 2023. <a href="https://arxiv.org/abs/2305.10387">Elaborative simplification as implicit questions under discussion</a>. Proceedings of EMNLP 2023.</p>

<p>S. Malik-Moraleda, K Mahowald, BR Conway, E Gibson. 2023. <a href="http://tedlab.mit.edu/tedlab_website/researchpapers/malik-moraleda_et_al_2023_psych_science.pdf">Concepts Are Restructured During Language Contact: The Birth of Blue and Other Color Concepts in Tsimane’-Spanish Bilinguals</a>. <em>Psychological Science</em>.</p>

<p>S. Malik-Moraleda, O. Jouravlev, Z. Mineroff, T. Cucu, M. Taliaferro, K. Mahowald, I. Blank, E. Fedorenko. 2024. <a href="https://pubmed.ncbi.nlm.nih.gov/38466812/">Functional characterization of the language network of polyglots and hyperpolyglots with precision fMRI</a>. <em>Cerebral Cortex</em>.</p>

<p>W. Liang, K. Mahowald, J. Raymond, V. Krishna, D. Smith, D. Jurafsky, D. McFarland, J. Zou. 2023. <a href="https://www.biorxiv.org/content/biorxiv/early/2023/01/04/2023.01.04.522708.full.pdf">How random is the review outcome? A systematic study of the impact of external factors on eLife peer review.</a></p>

<p>O. Jouravlev, K. Mahowald, A. Paunov, E. Gibson, E. Fedorenko. 2023. <a href="https://psyarxiv.com/es9wt/">Evaluation of Psychometric Properties and Inter-test Associations for Three Popular Measures of Social Competence</a>.</p>

<p>A. Diwan, L. Berry, E. Choi, D. Harwath, K. Mahowald. 2022. <a href="https://arxiv.org/pdf/2211.00768.pdf">Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality</a>. Proceedings of EMNLP 2022.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/fig1_kaushal.jpg" />In Kaushal and Mahowald, the input is a model embedding and we train
MLPs to classify whether a particular character (e.g., “a”) occurs in a particular token (e.g, “employee”).</span>A. Kaushal, K. Mahowald. 2022. <a href="https://arxiv.org/pdf/2206.02608">What do tokens know about their characters and how do they know it?</a>. Proceedings of NAACL 2022.</p>

<p>N. Rezaii, K. Mahowald, R. Ryskin, B. Dickerson, E. Gibson. 2022. 
<a href="http://tedlab.mit.edu/tedlab_website/researchpapers/Rezaiia,Mahowald,%20Ryskin,%20Dickersona%20&amp;%20Gibson%202022.pdf">A syntax–lexicon trade-off in language production</a>. 
<em>Proceedings of the National Academy of Sciences</em> 119 (25), e2120203119.</p>

<p>V. Kovatchev, T. Chatterjee, V. Govindarajan, J. Chen, E. Choi, G. Chronis, A. Das, K. Erk, M. Lease, J. Li, Y. Wu, K. Mahowald. 2022. <a href="https://arxiv.org/pdf/2206.14729.pdf">longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks</a>.
Proceedings of DADC 2022.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/oxlex.jpeg" />Oxford Handbook of the Mental Lexicon (with cover art by Blake).</span>K. Mahowald, I. Dautriche, M. Braginsky, E. Gibson. 2022. <a href="https://psyarxiv.com/4an6v">Efficient communication and the organization of the lexicon</a>. In <a href="https://www.amazon.com/Oxford-Handbook-Mental-Lexicon-Handbooks/dp/0198845006"><em>Oxford University Press Handbook of the Mental Lexicon</em></a>.</p>

<p>I. Papadimitriou, R. Futrell, K. Mahowald. 2022. <a href="https://arxiv.org/pdf/2203.06204.pdf">When classifying grammatical role, BERT doesn’t care about word order… except when it matters</a>. <em>ACL 2022</em>.</p>

<p>K. Mahowald, E. Diachek, E. Gibson, E. Fedorenko, R. Futrell. 2022. <a href="https://arxiv.org/pdf/2201.12911.pdf">Grammatical cues are largely, but not completely, redundant with word meanings in natural language</a>.</p>

<p>B. Schmidt, S.T. Piantadosi, K. Mahowald. 2021. <a href="assets/pdf/schmidt_piantadosi_mahowald.pdf">Uncontrolled corpus composition drives an apparent surge in cognitive distortions</a>. Letter to <em>PNAS</em> (in resposne to <a href="https://www.pnas.org/content/118/30/e2102061118">Bollen et al.</a>).</p>

<p>A. Jones, W. Wang, K. Mahowald. 2021. <a href="https://arxiv.org/pdf/2109.06324.pdf">A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space</a>. <em>EMNLP 2021</em>.</p>

<p>T. Pimentel, I. Nikkarinen, K. Mahowald, R. Cotterell, D. Blasi. 2021. <a href="https://arxiv.org/pdf/2104.14279.pdf">How (Non-) Optimal is the Lexicon?</a>. <em>NAACL 2021</em>.</p>

<p>J. Rozner, C. Potts, K. Mahowald. 2021. <a href="https://arxiv.org/pdf/2104.08620.pdf">Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP</a>. <em>NeurIPS 2021</em></p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/ergs.png" />Papadimitriou et al. on “Deep Subjecthood” trained probes on a subject vs. object classification task in multilingual BERT, transferring the performance of the probe across languages with different morphosyntactic alignment systems.</span>I. Papadimitriou, R. Futrell, E. Chi, K. Mahowald. 2021. <a href="https://arxiv.org/pdf/2101.11043.pdf">Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT</a>. <em>EACL 2021</em>.</p>

<p>ManyBabies2 Team. 2021 (submitted). <a href="https://pure.mpg.de/rest/items/item_3288327/component/file_3288328/content">Action anticipation based on an agent’s epistemic state in toddlers and adults</a>.</p>

<p>K. Mahowald, D. Jurafsky, M. Norris. 2021. <a href="https://ling.auf.net/lingbuzz/005788/current.pdf?_s=x7EYj8YTdvx2ZPYo">Concord begets concord: A Bayesian model of nominal concord typology</a>. <em>Proceedings of 95th LSA (2021)</em>.</p>

<p>D. Card, P. Henderson, U. Khandelwal, R. Jia,
K. Mahowald, D. Jurafsky. 2020. <a href="https://www.aclweb.org/anthology/2020.emnlp-main.745.pdf">With Little Power Comes Great Responsibility</a>. <em>EMNLP 2020</em>.</p>

<p>K. Mahowald, G. Kachergis, M.C. Frank. 2020. <a href="https://psyarxiv.com/ut86f/">What counts as an
  exemplar model, anyway? A commentary on Ambridge (2020)</a>. <em>First Language</em>.</p>

<p>E. Gibson, R. Futrell, S.T. Piantadosi, I. Dautriche, K. Mahowald, L. Bergen, R. Levy. 2019. <a href="http://colala.berkeley.edu/papers/gibson2019how.pdf">How efficiency shapes human language</a>. <em>Trends in Cognitive Science</em>.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/cor_freq_phon.png" />Mahowald et al. “Word Forms Are Structured for Efficient” showed a robust correlation, across almost 100 world languages, between orthographic probability and word frequency.</span> K. Mahowald, I. Dautriche, E. Gibson, S.T. Piantadosi. 2018. <a href="http://tedlab.mit.edu/tedlab_website/researchpapers/mahowald%20et%20al%202018.pdf">Word Forms Are Structured for Efficient Use</a>. <em>Cognitive Science</em>.</p>

<p>Z. Mineroff, I. Blank, K. Mahowald, E. Fedorenko. 2018. A robust dissociation among the language, multiple demand, and default mode networks: evidence from inter-region correlations in effect size. <em>Neuropsychologia</em>, 119, 501-511.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/color_paper.png" />Gibson et al. “Color naming across languages reflects color use” found that the information content of world color systems varied by language, but in all cases warm colors tended to be more information-rich than cool colors. In this plot, langauges are ordered by informativity of color system.</span> E. Gibson, R. Futrell, J. Jara-Ettinger, K. Mahowald, S. Ratnasingam, M. Gibson, S.T Piantadosi, B.R. Conway. 2017. <a href="http://tedlab.mit.edu/tedlab_website/researchpapers/gibson2017color.pdf">Color naming across languages reflects color use</a>. <em>PNAS</em>. 114 (40), 10785-10790.</p>

<p>Gibson, E., Tan, C., Futrell, R., Mahowald, K., Konieczny, L., Hemforth, B., Fedorenko, E. 2017. Don’t underestimate the benefits of being misunderstood. <em>Psychological science</em>, 28(6), 703-712.</p>

<p>I. Dautriche*, K. Mahowald*, E. Gibson, S.T. Piantadosi. 2017. <a href="http://tedlab.mit.edu/tedlab_website/researchpapers/dautriche2017words.pdf">Words cluster phonetically beyond phonotactic regularities.</a> <em>Cognition</em>, 163, 128-145.</p>

<p>I. Dautriche, K. Mahowald, E. Gibson, S.T. Piantadosi. 2017. <a href="http://tedlab.mit.edu/tedlab_website/researchpapers/dautriche2016wordform.pdf">Wordform
similarity increases with semantic similarity: an analysis of 100
languages</a>. <em>Cognitive Science</em>. doi: 10.1111/cogs.12453</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/meta_plot.png" />Mahowald et al. “A meta-analysis of Syntactic Priming” analyzed 71 syntactic priming papers and found a robust effect of syntactic priming that was greatly magnified by lexical boost. This plot shows model coefficients for various moderators of the priming effect.</span> K. Mahowald, A. James, R. Futrell, E. Gibson. 2016. <a href="assets/pdf/meta.pdf">A meta-analysis of syntactic priming</a>. <em>Journal of Memory and Language</em>, 91, 5-27.</p>

<p>K. Mahowald, A. James, R. Futrell, E. Gibson. 2017. Structural priming is most useful when the conclusions are statistically robust. <em>Behavioral and Brain Sciences</em>, 40.</p>

<p>K. Mahowald, E. Fedorenko. 2016. <a href="assets/pdf/neuroimage.pdf">Reliable individual-level neural markers of highlevel language processing: A necessary precursor for relating neural variability to behavioral and genetic variability</a>. 2016. <em>Neuroimage</em>, 139, 74-93. doi: 10.1016/j.neuroimage.2016.05.073.</p>

<p>I. Blank, E. Fedorenko, Z. Balewski, and K. Mahowald. 2016. Syntactic processing is distributed across the language network. <em>Neuroimage</em>, 127, 307-323.</p>

<p><span class="marginnote"><img class="fullwidth" src="assets/img/snap.png" />With SNAP Judgments, we show that small N experiments can be used to obtain quantitative linguistic data at low cost and with reliable results. This plot shows the distribution of effect sizes (which is large) for forced choice grammaticality judgments sampled from 100 examples in <em>Linguistic Inquiry</em>.</span>K. Mahowald, P. Graff, J. Hartman, and E. Gibson. 2016. <a href="assets/pdf/SNAP.pdf">SNAP Judgments: A Small N Acceptability Paradigm (SNAP) for Linguistic Acceptability Judgments</a>. <em>Language</em>, 92 (3), 619-635.</p>

<p>R. Futrell, K. Mahowald, E. Gibson. 2015. <a href="assets/pdf/dep.pdf">Large-Scale Evidence of Dependency Length Minimization in 37 Languages</a>. <em>Proceedings of the National Academy of Sciences</em>. Published online before print August 3, 2015. doi: 10.1073/pnas.1502134112 <sup class="sidenote-number">2</sup><span class="sidenote"><sup class="sidenote-number">2</sup> See popular discussion in <a href="http://news.sciencemag.org/social-sciences/2015/08/all-languages-have-evolved-have-common">Science Magazine</a> and on <br /> <a href="https://www.reddit.com/r/science/comments/3g0vpk/mit_claims_to_have_found_a_language_universal/">Reddit</a></span><span class="marginnote"><img class="fullwidth" src="assets/img/depling.png" />Futrell, Mahowald, Gibson (2015): The dependency lengths in natural languages are minimized relative to random baseline.</span></p>

<p>R. Futrell, K. Mahowald, E. Gibson. 2015. <a href="assets/pdf/depling-conf.pdf">Quantifying Word Order Freedom in Dependency Corpora</a>. Proceedings of Depling 2015.</p>

<p>R. Singh, E. Fedorenko, K. Mahowald, E. Gibson. 2015. Presupposition accommodation is costly only in implausible contexts. <em>Cognitive Science</em>. Published online before print July, 2015. doi: 10.1111/cogs.12260</p>

<p>E. Gibson, P. Jacobson, P. Graff, E. Fedorenko, K. Mahowald, S.T. Piantadosi. 2014. <a href="assets/pdf/gibsonetal_jsemantics.pdf">A pragmatic account of complexity in definite Antecedent-Contained-Deletion relative clauses</a>. <em>Journal of Semantics</em>. Published online before print.</p>

<p>K. Mahowald and E. Gibson. 2013. <a href="assets/pdf/mahowald_pnas.pdf">Short, frequent words are more likely to appear genetically related by chance</a>. Letter to <em>Proceedings of the National Academy of Sciences</em>.<sup class="sidenote-number">3</sup><span class="sidenote"><sup class="sidenote-number">3</sup> See original article by Pagel et al. <a href="http://www.pnas.org/content/110/21/8471.abstract">here</a> and their reply <br /> <a href="assets/img/pagel_reply.pdf">here</a>.</span></p>

<p>K. Mahowald, E. Fedorenko, S.T. Piantadosi, and E. Gibson. 2013. <a href="assets/pdf/mahowald_info.pdf">Info/information theory: speakers choose shorter words in predictive contexts</a>. <em>Cognition</em>, 126, 313-318.</p>

<p>K. Mahowald. 2012. <a href="assets/pdf/mahowald_naive_bayes_shax.pdf">A Naive Bayes Classifier for Shakespeare’s second person pronoun</a>. <em>Literary and Linguistic Computing</em>. 27 (1): 17-23. doi:10.1093/llc/fqr045</p>

<p>K. Mahowald. 2010. <a href="assets/pdf/chaucer.pdf">It may nat be: Chaucer, Derrida, and the Impossibility of the Gift</a>. <em>Studies in the Age of Chaucer</em>. 32:129-150.</p>

<p>K. Mahowald. 2011. <a href="assets/pdf/lfg11mahowald.pdf">An LFG Account of Word Order Freezing</a>. In M. Butt and T. H. King, eds., Proceedings of the LFG11 Conference. Hong Kong: CSLI Publications: http://www-csli.stanford.edu/publications.</p>

<!-- Default Statcounter code for KylePage ds -->
<script type="text/javascript">
var sc_project=8927616; 
var sc_invisible=1; 
var sc_security="be6c0554"; 
</script>

<script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async=""></script>

<noscript><div class="statcounter"><a title="hit counter" href="https://statcounter.com/" target="_blank"><img class="statcounter" src="https://c.statcounter.com/8927616/0/be6c0554/1/" alt="hit counter" /></a></div></noscript>
<!-- End of Statcounter Code -->



    </article>
    <span class="print-footer"> - Kyle Mahowald</span>
    <footer>
  <ul class="footer-links group">
    <li><a href="mailto:kyle@utexas.edu"><span class="icon-mail"></span></a></li>    
      
  </ul>
<div class="credits">
<span>&copy; 2025 &nbsp;&nbsp;KYLE MAHOWALD</span></br> 
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for Content-centric blogging </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>

  </body>
</html>
